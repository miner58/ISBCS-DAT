import os
import glob
import pandas as pd
from ray import tune
from tune_train.base_train_func import test_func
from tune_train.groupDRO_train_func import test_func as groupDRO_test_func
from tune_train.base_train_func import train_func
from tune_train.groupDRO_train_func import train_func as groupDRO_train_func
from runner_raytune import EEGDARayTuneRunner

def make_data_config_path_dict(data_config_base_dir, data_config_path_dict):
    for folder_name in data_config_path_dict.keys():
        folder_path = os.path.join(data_config_base_dir, folder_name)
        json_files = glob.glob(os.path.join(folder_path, '**', '*.json'), recursive=True)
        data_config_path_dict[folder_name] = json_files
    print(data_config_path_dict)

    return data_config_path_dict


class EEGDARayTuneAnalyzer(EEGDARayTuneRunner):
    def __init__(self, config_path, experiment_path, 
                 train_func, test_func, test_data_default_path, data_config_path_dict):
        """
        Initializes the EEGDARayTuneAnalyzer class.

        Args:
            experiment_path (str): The path to the Ray Tune experiment.
            test_func (callable): The function to test the model on the test data.
        """
        super(EEGDARayTuneAnalyzer, self).__init__(config_path, train_func=train_func)
        self.experiment_path = experiment_path
        self.test_func = test_func
        self.results = None
        self.checkpoint_metric = None
        self.test_data_default_path = test_data_default_path
        self.data_config_path_dict=data_config_path_dict
        self.output_path = os.path.join( os.path.dirname(self.experiment_path), 'analyzing_result')
        os.makedirs(self.output_path, exist_ok=True)

    def set_config_dict(self, config_dict):
        self.config_dict = config_dict


    def load_tuning_results(self):
        """
        Loads the tuning results from the experiment path.
        """
        self.tunner_setup()
        scheduler_config = self.ray_tune_config['tune_parameters']['scheduler']
        self.checkpoint_metric = scheduler_config['parameters']['metric']

        print(f"Loading results from {self.experiment_path}...")
        restored_tuner = tune.Tuner.restore(self.experiment_path, self.trainer)
        self.results = restored_tuner.get_results()

    def print_results(self):
        """
        Prints the results of the tuning process.
        """
        if not self.results:
            print("No results loaded.")
            return

        for i, result in enumerate(self.results):
            if result.error:
                print(f"Trial #{i} had an error:", result.error)
                continue
            print(f"Trial #{i} finished successfully with a mean accuracy metric of:",
                  result.metrics[self.checkpoint_metric])

    def run_test_func(self):
        """
        Runs the test function on the best checkpoints of each trial.
        """
        if not self.results:
            print("No results found. Please load the tuning results first.")
            return

        best_result_df = self.results.get_dataframe(filter_metric=self.checkpoint_metric, filter_mode="max")
        best_result_df.to_csv(os.path.join(self.output_path, 'best_result_df.csv'))

        combined_all_df = pd.DataFrame()
        for i, result in enumerate(self.results):
            test_results_df = self._run_test_on_best_checkpoint(result, best_result_df.iloc[i])
            combined_all_df = pd.concat([combined_all_df, test_results_df], ignore_index=True)

        combined_all_df.to_csv(os.path.join(self.output_path, 'raw_test_results.csv'), index=False)
    
    def extract_best_checkpoint(self):
        """
        Extracts the best checkpoint from each trial's results.
        """
        def validate_checkpoint_path(checkpoint_path):
            if not os.path.exists(checkpoint_path):
                raise FileNotFoundError(f"Checkpoint path does not exist: {checkpoint_path}")
            return checkpoint_path
        if not self.results:
            print("No results found. Please load the tuning results first.")
            return

        best_checkpoints = []
        for i, result in enumerate(self.results):
            best_checkpoint = result.get_best_checkpoint(self.checkpoint_metric, "max")
            validate_checkpoint_path(best_checkpoint.path)
            best_checkpoints.append(best_checkpoint.path)

        # Save the best checkpoints to a CSV file
        # column: ['path']
        best_checkpoints = [{'path': path} for path in best_checkpoints]
        best_checkpoints_df = pd.DataFrame(best_checkpoints)
        best_checkpoints_df.to_csv(os.path.join(self.output_path, 'best_checkpoints.csv'), index=False)


    def _run_test_on_best_checkpoint(self, result, best_row):
        """
        Runs the test function on the best checkpoint of a single trial.

        Args:
            result: The result of a single trial.
            best_row: The corresponding row from the best result dataframe.

        Returns:
            DataFrame containing test results for the current trial.
        """
        best_checkpoint = result.get_best_checkpoint(self.checkpoint_metric, "max")
        dir_name = os.path.dirname(best_checkpoint.path)

        # Extracting best checkpoint elements
        filtered_elements = list(filter(lambda x: x[0] == best_checkpoint, result.best_checkpoints))[0]
        best_checkpoint = filtered_elements[0]
        best_config = filtered_elements[1]

        # Running tests for each subject configuration
        test_results_list = []
        for subject_name, sub_config_path_list in self.data_config_path_dict.items():
            for sub_config_path in sub_config_path_list:
                test_results = self.test_func(best_config['config']['train_loop_config'], best_checkpoint.path, sub_config_path, self.test_data_default_path)
                test_result = test_results[0]
                test_result['test_subject_name'] = subject_name
                test_result['test_config_path'] = sub_config_path
                test_results_list.append(test_result)

        # Combine the results with the best result row
        test_results_df = pd.DataFrame(test_results_list)
        combined_row = pd.concat([best_row.to_frame().T.reset_index(drop=True)] * len(test_results_df), ignore_index=True)
        combined_row = pd.concat([combined_row, test_results_df.reset_index(drop=True)], axis=1)
        # for _, test_res in test_results_df.iterrows():
        #     combined_row = pd.concat([best_row.to_frame().T.reset_index(drop=True), test_res.to_frame().T.reset_index(drop=True)], axis=1)
        return combined_row

    def analyze_results(self):
        """
        Analyzes the results of the tuning, typically by selecting the best configuration
        and testing on the test data.
        """
        if not self.results:
            print("No results found. Please load the tuning results first.")
            return

        # Extract the best result
        best_result = self.results.get_best_result(metric=self.checkpoint_metric, mode="max")
        print(f"Best result: {best_result}")

        # Perform the test with the best configuration on the test data
        best_checkpoint = best_result.checkpoint
        test_data = self.test_func(best_checkpoint)
        print("Test Results:", test_data)

def extract_max_acc_rows(csv_path, output_path, group_cols=[], metric="test/report/macro avg/accuracy"):
    """
    CSV íŒŒì¼ì—ì„œ test_target_acc(ë˜ëŠ” test_acc)ê°€ ê°€ì¥ ë†’ì€ í–‰ë§Œ ì¶”ì¶œí•˜ì—¬,
    subject_nameì´ ìˆìœ¼ë©´ (subject_name, test_subject_name) ê·¸ë£¹ë³„ë¡œ,
    ì—†ìœ¼ë©´ test_subject_name ê·¸ë£¹ë³„ë¡œ ìµœëŒ€ê°’ì„ ì°¾ëŠ”ë‹¤.
    ê²°ê³¼ëŠ” ê¸°ì¡´ csv_path(ì›ë³¸ íŒŒì¼ëª…)ë¡œ ë®ì–´ì“´ë‹¤.
    """

    # metric = "test/report/macro avg/accuracy"
    # metric = "test_target_acc"

    # Load the CSV file
    df = pd.read_csv(csv_path)
    
    # Rename columns for consistency
    df = df.rename(columns={
        'config/train_loop_config/grl_lambda': 'grl_lambda',
        'config/train_loop_config/lnl_lambda': 'lnl_lambda',
        'config/train_loop_config/subject_name': 'subject_name',
        'test_subject_name' : 'test_subject_name',
        'test_acc': 'test_target_acc',
        'test_f1': 'test_target_f1'
    })
    
    # Check if 'test_target_acc' exists
    if metric not in df.columns:
        print(f"{metric} column not found in the data.")
        return

    # subject_name ì»¬ëŸ¼ ê°’ì´ A, B, C ë“±ìœ¼ë¡œ ë˜ì–´ ìˆìœ¼ë©´ groupA, groupB, groupCë¡œ ë³€ê²½
    if 'subject_name' in df.columns:
        df['subject_name'] = df['subject_name'].replace({'A': 'groupA', 'B': 'groupB', 'C': 'groupC'})
    
    # subject_name ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (NaNë§Œ ìˆëŠ”ì§€ ì—¬ë¶€ë„ í•¨ê»˜ ì²´í¬í•  ìˆ˜ë„ ìˆìŒ)
    has_subject_name = ('subject_name' in df.columns)
    
    if has_subject_name and not df['subject_name'].isnull().all():
        # subject_name ì´ ìˆê³ , ì „ë¶€ NaNì´ ì•„ë‹ˆë¼ë©´ (subject_name, test_subject_name)ë³„ ìµœëŒ€ accë¥¼ ì°¾ëŠ”ë‹¤.
        df = df[df['subject_name'] == df['test_subject_name']]
        group_cols.extend(['subject_name', 'test_subject_name'])
    else:
        # subject_nameì´ ì—†ê±°ë‚˜, ì „ë¶€ NaNì´ë©´ ê¸°ì¡´ ë¡œì§: test_subject_nameë³„ ìµœëŒ€ acc
        group_cols.extend(['test_subject_name'])

    # groupby í›„ idxmaxë¥¼ ì´ìš©í•´ test_target_accê°€ ê°€ì¥ í° í–‰ì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ëŠ”ë‹¤.
    max_acc_indices = df.groupby(group_cols)[metric].idxmax()
    
    # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ í–‰ë§Œ ì¶”ì¶œ
    max_acc_df = df.loc[max_acc_indices]
    
    if 'subject_name' in max_acc_df.columns:
        if 'config/train_loop_config/day_combination' in max_acc_df.columns:
            max_acc_df = max_acc_df.sort_values(by=['subject_name', 'config/train_loop_config/day_combination'])
        else:
            max_acc_df = max_acc_df.sort_values(by=['subject_name'])
    else:
        # subject_nameì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ê·¸ë£¹ ì»¬ëŸ¼ ê¸°ì¤€ ì •ë ¬
        max_acc_df = max_acc_df.sort_values(by=group_cols)
    
    # ê¸°ì¡´ csv_path íŒŒì¼ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë®ì–´ì“°ê¸°)
    # ì¶œë ¥ ê²½ë¡œë§Œ ë‹¤ë¥´ê²Œ ì§€ì •í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì—ì„œ csv_path ëŒ€ì‹  ë‹¤ë¥¸ ê²½ë¡œë¥¼ ì“°ë©´ ë¨
    output_csv = os.path.join(output_path, f'max_metric_per_subject.csv')

    # ê²°ê³¼ë¥¼ ì €ì¥ (ë®ì–´ì“°ê¸°)
    max_acc_df.to_csv(output_csv, index=False)
    
    print(f"Rows with maximum metric per group saved (overwritten) to {output_csv}")

def extract_max_acc_rows_average(csv_path, output_path, metric="test/report/macro avg/accuracy"):
    """
    CSV íŒŒì¼ì—ì„œ groupA, groupB, groupCì˜ í‰ê·  ì„±ëŠ¥ì´ ê°€ì¥ ë†’ì€ ì„¤ì •ì„ ì¶”ì¶œí•œë‹¤.
    ê° ì„¤ì •(configuration)ì— ëŒ€í•´ ì„¸ ê·¸ë£¹ì˜ í‰ê·  ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³ , ê°€ì¥ ë†’ì€ í‰ê· ì„ ê°€ì§„ ì„¤ì •ì˜ ëª¨ë“  í–‰ì„ ì¶”ì¶œí•œë‹¤.
    """
    
    df = pd.read_csv(csv_path)
    
    # Rename columns for consistency
    df = df.rename(columns={
        'config/train_loop_config/grl_lambda': 'grl_lambda',
        'config/train_loop_config/lnl_lambda': 'lnl_lambda',
        'config/train_loop_config/lr': 'lr',
        'config/train_loop_config/subject_name': 'subject_name',
        'test_subject_name': 'test_subject_name',
        'test_acc': 'test_target_acc',
        'test_f1': 'test_target_f1'
    })
    
    if metric not in df.columns:
        print(f"{metric} column not found in the data.")
        return
    
    # subject_name ì»¬ëŸ¼ ê°’ ì •ê·œí™”
    if 'subject_name' in df.columns:
        df['subject_name'] = df['subject_name'].replace({'A': 'groupA', 'B': 'groupB', 'C': 'groupC'})
    
    # ì„¤ì • ì‹ë³„ì„ ìœ„í•œ ì»¬ëŸ¼ë“¤ lr, grl_lambda, lnl_lambda
    config_cols = [col for col in df.columns if col in ['grl_lambda', 'lnl_lambda', 'lr']]
    if 'subject_name' in df.columns:
        config_cols.append('subject_name')
    
    # config/train_loop_config/lr ì»¬ëŸ¼ë„ ì¶”ê°€ (renameë˜ì§€ ì•Šì€ ê²½ìš°)
    if 'config/train_loop_config/lr' in df.columns and 'config/train_loop_config/lr' not in config_cols:
        config_cols.append('config/train_loop_config/lr')
    
    # Debug: ì„¤ì • ì •ë³´ ì¶œë ¥
    print(f"Config columns found: {config_cols}")
    print(f"DataFrame shape: {df.shape}")
    print(f"Available columns: {list(df.columns)}")
    
    # ë¹ˆ config_cols ì²˜ë¦¬
    if not config_cols:
        print("No configuration columns found. Using trial_id or index as grouping.")
        # trial_idë‚˜ ë‹¤ë¥¸ ì‹ë³„ìê°€ ìˆëŠ”ì§€ í™•ì¸
        if 'trial_id' in df.columns:
            config_cols = ['trial_id']
        else:
            # ê³ ìœ í•œ í–‰ì„ ì‹ë³„í•˜ê¸° ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
            df['config_index'] = df.index
            config_cols = ['config_index']
    
    # ê° ì„¤ì •ë³„ë¡œ ê·¸ë£¹ í‰ê·  ê³„ì‚°
    avg_performance = df.groupby(config_cols)[metric].mean().reset_index()
    avg_performance.columns = config_cols + ['avg_metric']
    
    # Debug: í‰ê·  ì„±ëŠ¥ ì •ë³´ ì¶œë ¥
    print(f"Average performance shape: {avg_performance.shape}")
    print(f"Average performance head:\n{avg_performance.head()}")
    
    # ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
    if avg_performance.empty or avg_performance['avg_metric'].isna().all():
        print("No valid configurations found or all metrics are NaN.")
        return
    
    # ê°€ì¥ ë†’ì€ í‰ê·  ì„±ëŠ¥ì„ ê°€ì§„ ì„¤ì • ì°¾ê¸°
    best_config_idx = avg_performance['avg_metric'].idxmax()
    best_config = avg_performance.iloc[best_config_idx]
    
    # í•´ë‹¹ ì„¤ì •ì˜ ëª¨ë“  í–‰ ì¶”ì¶œ
    mask = pd.Series([True] * len(df))
    for col in config_cols:
        if col in df.columns:
            mask &= (df[col] == best_config[col])
    
    best_avg_df = df[mask].copy()
    best_avg_df = best_avg_df.sort_values(by=['test_subject_name'])
    
    # í‰ê·  ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
    best_avg_df['average_metric'] = best_config['avg_metric']
    
    output_csv = os.path.join(output_path, 'max_average_metric_config.csv')
    best_avg_df.to_csv(output_csv, index=False)
    
    print(f"Configuration with highest average metric ({best_config['avg_metric']:.4f}) saved to {output_csv}")
    return best_avg_df

def main(experiment_path_list, test_data_default_path, data_config_base_dir, data_type, train_func, test_func):
    for experiment_path in experiment_path_list:
        print(f"\n=== Processing experiment: {experiment_path} ===")
        
        # Cross-domain ì‹¤í—˜ ì—¬ë¶€ í™•ì¸
        if is_cross_domain_experiment(experiment_path):
            print("ğŸ”„ Cross-domain experiment detected - using ALL config")
            data_config_path_dict = make_cross_domain_data_config_dict(data_type, data_config_base_dir)
        else:
            print("ğŸ  Inner experiment detected - using individual group configs")
            data_config_path_dict = {"groupA": [], "groupB": [], "groupC": []}
            data_config_path_dict = make_data_config_path_dict(data_config_base_dir, data_config_path_dict)
        
        # Find the .yml file in the experiment_path directory
        config_path = glob.glob(os.path.join(experiment_path, "*.yml"))[0]
        if not config_path:
            raise FileNotFoundError(f"No .yml file found in the directory: {experiment_path}")

        # Initialize the analyzer
        analyzer = EEGDARayTuneAnalyzer(config_path, os.path.join(experiment_path, 'eeg_tune'), 
                                        train_func, test_func, test_data_default_path, data_config_path_dict)

        # Load results and analyze them
        analyzer.load_tuning_results()
        analyzer.print_results()
        analyzer.run_test_func()

        extract_max_acc_rows(os.path.join(experiment_path, 'analyzing_result','raw_test_results.csv'),
                os.path.join(experiment_path, 'analyzing_result'), group_cols=[], metric="test/report/macro avg/accuracy")
        
        # Extract configuration with highest average performance across groups
        extract_max_acc_rows_average(os.path.join(experiment_path, 'analyzing_result','raw_test_results.csv'),
                os.path.join(experiment_path, 'analyzing_result'), metric="test/report/macro avg/accuracy")

def extract_best_checkpoint_for_each_experiment(experiment_path_list):
    """
    Extracts the best checkpoint for each experiment and saves it to a CSV file.
    """
    for experiment_path in experiment_path_list:
        config_path = glob.glob(os.path.join(experiment_path, "*.yml"))[0]
        analyzer = EEGDARayTuneAnalyzer(config_path, os.path.join(experiment_path, 'eeg_tune'), 
                                        train_func, test_func, test_data_default_path, data_config_path_dict)
        analyzer.load_tuning_results()
        analyzer.extract_best_checkpoint()

"""
filter_experiment_path_list í•¨ìˆ˜ëŠ” 
data_typeì— ë”°ë¼ ì‹¤í—˜ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•„í„°ë§í•œë‹¤.
data_type = "wire" ì¼ ë•Œ, 
    - 2_innrer_wire, 3_wireless2wire ê²½ë¡œë§Œ í•„í„°ë§
data_type = "wireless" ì¼ ë•Œ, 
    - 1_inner_wireless, 4_wire2wireless ê²½ë¡œë§Œ í•„í„°ë§
"""
def filter_experiment_path_list(experiment_path_list, data_type):
    if data_type == "wire":
        return [path for path in experiment_path_list if "2_inner_wire" in path or "3_wireless2wire" in path]
    elif data_type == "wireless":
        return [path for path in experiment_path_list if "1_inner_wireless" in path or "4_wire2wireless" in path]
    if data_type == "UI":
        return [path for path in experiment_path_list if "1_inner_UI" in path or "4_UNM2UI" in path]
    elif data_type == "UNM":
        return [path for path in experiment_path_list if "2_inner_UNM" in path or "3_UI2UNM" in path]
    else:
        raise ValueError(f"Invalid data_type: {data_type}")

def filter_experiment_path_list_ray_results(experiment_path_list):
    """
    ray_results ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ë¡œë§Œ í•„í„°ë§í•œë‹¤.
    """
    return [path for path in experiment_path_list if "ray_results" in path]

"""
for tmp_folder in tmp_folder_list:
        # tmp_folder ë‚´ë¶€ì˜ í´ë” ëª…ì„ ì¶”ê°€
        tmp_dir = os.listdir(tmp_folder)
        experiment_path_list.extend([os.path.join(tmp_folder, folder) for folder in tmp_dir])
ìœ„ì˜ ì½”ë“œë¥¼ ì¼ë°˜í™” ì‹œí‚¤ê³  í•¨ìˆ˜í™” í•˜ì˜€ìŒ
"""
def get_sub_folder_list(folder_list):
    sub_folder_list = []
    for folder in folder_list:
        print(f">> folder: {folder}")
        tmp_dir = os.listdir(folder)
        print(f"tmp_dir:\n {tmp_dir}")
        sub_folder_list.extend([os.path.join(folder, folder_name) for folder_name in tmp_dir])
    return sub_folder_list

def is_cross_domain_experiment(experiment_path):
    """
    Cross-domain ì‹¤í—˜ì¸ì§€ íŒë³„í•˜ëŠ” í•¨ìˆ˜
    3_UI2UNM ë˜ëŠ” 4_UNM2UIê°€ ê²½ë¡œì— í¬í•¨ë˜ë©´ True ë°˜í™˜
    """
    return "3_UI2UNM" in experiment_path or "4_UNM2UI" in experiment_path

def make_cross_domain_data_config_dict(data_type, data_config_base_dir):
    """
    Cross-domain ì‹¤í—˜ìš© ALL ì„¤ì • íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” data_config_path_dict ìƒì„±
    """
    if data_type == "UI":
        all_config_path = os.path.join(data_config_base_dir, "ALL", "dataConfigStimGRL_ALL.json")
        if os.path.exists(all_config_path):
            return {"ALL": [all_config_path]}
        else:
            print(f"Warning: ALL config file not found at {all_config_path}")
            return {"ALL": []}
    elif data_type == "UNM":
        # UNM íŒŒì¼ì—ì„œ UI ë°ì´í„°ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²½ìš° (4_UNM2UI)
        ui_config_base_dir = data_config_base_dir.replace("UNMconfig", "UIconfig")
        all_config_path = os.path.join(ui_config_base_dir, "ALL", "dataConfigStimGRL_ALL.json")
        if os.path.exists(all_config_path):
            return {"ALL": [all_config_path]}
        else:
            print(f"Warning: UI ALL config file not found at {all_config_path}")
            return {"ALL": []}
    return {"ALL": []}

if __name__ == "__main__":
    # # wire
    # data_type = "wire"
    # test_data_default_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization"
    # data_config_base_dir = "/home/jsw/Fairness/tmp/Fairness_for_generalization/data/raw3config/test/only1Day1,8"
    # data_config_path_dict = {"B202": [],
    #                         "N201": [],
    #                         "R203": []}

    # # wireless
    # data_type = "wireless"
    # test_data_default_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization"
    # data_config_base_dir = "/home/jsw/Fairness/tmp/Fairness_for_generalization/data/raw5&6config/test/only1Day1,8"
    # data_config_path_dict={"B112":[],
    #                   "N304":[],
    #                   "N310":[]}
    
    # UI
    data_type = "UI"
    test_data_default_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization"
    data_config_base_dir = "/home/jsw/Fairness/tmp/Fairness_for_generalization/data/UIconfig/test"
    data_config_path_dict={"groupA":[],
                      "groupB":[],
                      "groupC":[]}

    # # UNM
    # data_type = "UNM"
    # test_data_default_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization"
    # data_config_base_dir = "/home/jsw/Fairness/tmp/Fairness_for_generalization/data/UNMconfig/test"
    # data_config_path_dict={"groupA":[],
    #                   "groupB":[],
    #                   "groupC":[]}

    tmp_folder_list = [
        "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_public_20",
        "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_public_sub_60"
    ]

    experiment_path_list = [
    ]
    experiment_path_list = get_sub_folder_list(tmp_folder_list)

    experiment_path_list = filter_experiment_path_list(experiment_path_list, data_type)

    experiment_path_list = get_sub_folder_list(experiment_path_list)

    experiment_path_list = filter_experiment_path_list_ray_results(experiment_path_list)

    experiment_path_list = [
        "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_public_20/4_UNM2UI/ray_results_allMouse_UNM_finetune_ReduceLROnPlateau_batch16",
        "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_public_sub_60/4_UNM2UI/ray_results_allMouse_UNM_finetune_ReduceLROnPlateau_LNL_batch16"
    ]

    # extract_best_checkpoint_for_each_experiment(experiment_path_list)
    main(experiment_path_list, test_data_default_path, data_config_base_dir, data_type, train_func=train_func, test_func=test_func)
    # main(experiment_path_list, test_data_default_path, data_config_base_dir, data_type, train_func=groupDRO_train_func, test_func=groupDRO_test_func)
