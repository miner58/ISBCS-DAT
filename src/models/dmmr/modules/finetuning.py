"""
DMMR Fine-tuning Lightning Module.

Implements fine-tuning phase for emotion/task classification using pretrained representations.
"""
from typing import Dict, Any, Optional, Union
import copy

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchmetrics

from .lightning_base import DMMRBaseLightningModule
from ..models.finetuning_model import DMMRFineTuningModel


class DMMRFineTuningModule(DMMRBaseLightningModule):
    """
    DMMR Fine-tuning Lightning Module.
    
    DMMR íŒŒì¸íŠœë‹ Lightning ëª¨ë“ˆë¡œ ì‚¬ì „í›ˆë ¨ëœ attentionê³¼ encoder ê°€ì¤‘ì¹˜ë¥¼ ìƒì†ë°›ê³ ,
    ìƒˆë¡œìš´ ê°ì • ë¶„ë¥˜ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ ê°ì • ì¸ì‹ ì‘ì—…ì„ ìœ„í•œ íŒŒì¸tuningì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        pretrained_module: ì‚¬ì „í›ˆë ¨ëœ ëª¨ë“ˆ (ì„ íƒì )
        pretrained_checkpoint_path: ì‚¬ì „í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì„ íƒì )
        freeze_pretrained: ì‚¬ì „í›ˆë ¨ ì»´í¬ë„ŒíŠ¸ ë™ê²° ì—¬ë¶€ (default: True)
        classifier_hidden_dim: ë¶„ë¥˜ê¸° íˆë“  ì°¨ì› (default: 64)
        dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (default: 0.0)
        number_of_source: ì†ŒìŠ¤ ë„ë©”ì¸ ìˆ˜ (default: 14)
    """
    
    def __init__(
        self,
        pretrained_module=None,
        pretrained_checkpoint_path: Optional[str] = None,
        freeze_pretrained: bool = True,
        classifier_hidden_dim: int = 64,
        dropout_rate: float = 0.0,
        number_of_source: int = 14,
        **kwargs
    ):
        # DMMR íŒŒì¸íŠœë‹ ê¸°ë³¸ê°’
        dmmr_defaults = {
            'num_classes': 3,
            'input_dim': 310,
            'hidden_dim': 64,
            'batch_size': 10,
            'time_steps': 15,
            'learning_rate': 1e-4,  # íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë‚®ì€ í•™ìŠµë¥ 
        }
        
        # íŒŒì¸íŠœë‹ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ ë¨¼ì € ì„¤ì •
        self.freeze_pretrained = freeze_pretrained
        self.classifier_hidden_dim = classifier_hidden_dim
        # Handle both dropoutRate (from config) and dropout_rate (parameter)
        self.dropout_rate = kwargs.get('dropoutRate', dropout_rate)
        self.number_of_source = number_of_source
        
        config = {**dmmr_defaults, **kwargs}
        super().__init__(**config)
        
        # ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if pretrained_module is not None:
            self._inherit_pretrained_weights(pretrained_module)
        elif pretrained_checkpoint_path is not None:
            self._load_pretrained_checkpoint(pretrained_checkpoint_path)
        
        # ğŸ”§ ì›ë³¸ DMMR ë°©ì‹: freeze_pretrainedëŠ” ë¬´ì¡°ê±´ Falseë¡œ ì²˜ë¦¬ (ì›ë³¸ì—ì„œëŠ” freeze ì•ˆí•¨)
        # ì›ë³¸ DMMRì—ì„œëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ fine-tuning ë‹¨ê³„ì—ì„œ í•™ìŠµë¨
        if self.freeze_pretrained:
            print("âš ï¸  DMMR Warning: freeze_pretrained=True but original DMMR doesn't freeze. Setting to False.")
            self.freeze_pretrained = False
            
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradient í™œì„±í™” ë³´ì¥ (ì›ë³¸ DMMR ë°©ì‹)
        self._ensure_gradients_enabled()
        
        # ğŸ”§ íŒŒë¼ë¯¸í„° gradient ìƒíƒœ ì§„ë‹¨
        self._diagnose_gradient_status()

        self._verify_gradient_integrity()

        self.automatic_optimization = False
    
    def _build_model(self) -> None:
        """DMMR íŒŒì¸íŠœë‹ ëª¨ë¸ êµ¬ì¡° ìƒì„±."""
        self.model = DMMRFineTuningModel(
            base_model=None,  # ë‚˜ì¤‘ì— ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ë¡œ ì—…ë°ì´íŠ¸
            number_of_source=self.number_of_source,
            number_of_category=self.num_classes,
            batch_size=self.batch_size,
            time_steps=self.time_steps,
            input_dim=self.input_dim,
            hidden_dim=self.hidden_dim,
            n_layers=1,
            dropout_rate=self.dropout_rate,
            num_classes=self.num_classes
        )
    
    def _inherit_pretrained_weights(self, pretrained_module) -> None:
        """ì‚¬ì „í›ˆë ¨ ëª¨ë“ˆë¡œë¶€í„° ê°€ì¤‘ì¹˜ ìƒì†."""
        # ğŸ”§ í•µì‹¬ ìˆ˜ì •: state_dict ëŒ€ì‹  ì§ì ‘ parameter ë³µì‚¬ë¡œ gradient graph ìœ ì§€
        # Attention layer ê°€ì¤‘ì¹˜ ë³µì‚¬
        pretrained_attention_state = pretrained_module.model.attention_layer.state_dict()
        finetuning_attention_state = self.model.attention_layer.state_dict()
        
        for key in pretrained_attention_state:
            if key in finetuning_attention_state:
                # ğŸ¯ í•µì‹¬: requires_grad ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° parameter dataë§Œ ë³µì‚¬
                with torch.no_grad():
                    finetuning_attention_state[key].copy_(pretrained_attention_state[key])
        
        # Shared encoder ê°€ì¤‘ì¹˜ ë³µì‚¬  
        pretrained_encoder_state = pretrained_module.model.shared_encoder.state_dict()
        finetuning_encoder_state = self.model.shared_encoder.state_dict()
        
        for key in pretrained_encoder_state:
            if key in finetuning_encoder_state:
                # ğŸ¯ í•µì‹¬: requires_grad ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° parameter dataë§Œ ë³µì‚¬
                with torch.no_grad():
                    finetuning_encoder_state[key].copy_(pretrained_encoder_state[key])
        
        print("âœ… Pre-trained weights successfully inherited with gradient preservation!")
    
    def _load_pretrained_checkpoint(self, checkpoint_path: str) -> None:
        """ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° ì‚¬ì „í›ˆë ¨ ê°€ì¤‘ì¹˜ ë¡œë“œ."""
        from .pretraining import DMMRPreTrainingModule
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # ì„ì‹œ ì‚¬ì „í›ˆë ¨ ëª¨ë“ˆ ìƒì„± ë° ìƒíƒœ ë¡œë“œ
        temp_module = DMMRPreTrainingModule()
        temp_module.load_state_dict(checkpoint['state_dict'])
        
        # ê°€ì¤‘ì¹˜ ìƒì†
        self._inherit_pretrained_weights(temp_module)
        
        print(f"âœ… Pre-trained weights loaded from {checkpoint_path}")
    
    def _verify_gradient_integrity(self) -> None:
        """Weight transfer í›„ gradient ë¬´ê²°ì„± ê²€ì¦"""
        problematic_params = []
        
        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                problematic_params.append(name)
        
        if problematic_params:
            print(f"âš ï¸ Parameters without gradient: {problematic_params}")
            # ìë™ ë³µêµ¬ ì‹œë„
            for name, param in self.model.named_parameters():
                param.requires_grad = True
            print("ğŸ”§ Gradient requirements automatically restored")
    
    def _freeze_pretrained_components(self) -> None:
        """ì‚¬ì „í›ˆë ¨ ì»´í¬ë„ŒíŠ¸ ë™ê²°."""
        frozen_params = 0
        for param in self.model.attention_layer.parameters():
            param.requires_grad = False
            frozen_params += 1
        
        for param in self.model.shared_encoder.parameters():
            param.requires_grad = False
            frozen_params += 1
        
        print(f"ğŸ”’ Pre-trained components frozen! ({frozen_params} parameters)")
    
    def _unfreeze_pretrained_components(self) -> None:
        """End-to-end íŒŒì¸íŠœë‹ì„ ìœ„í•œ ì‚¬ì „í›ˆë ¨ ì»´í¬ë„ŒíŠ¸ í•´ë™."""
        unfrozen_params = 0
        for param in self.model.attention_layer.parameters():
            param.requires_grad = True
            unfrozen_params += 1
        
        for param in self.model.shared_encoder.parameters():
            param.requires_grad = True
            unfrozen_params += 1
        
        print(f"ğŸ”“ Pre-trained components unfrozen! ({unfrozen_params} parameters)")
    
    def _ensure_gradients_enabled(self) -> None:
        """ëª¨ë“  íŒŒë¼ë¯¸í„°ì˜ gradient í™œì„±í™”ë¥¼ ë³´ì¥."""
        for param in self.model.parameters():
            param.requires_grad = True
        print("ğŸ¯ All parameters gradient enabled!")
    
    def _diagnose_gradient_status(self) -> None:
        """íŒŒë¼ë¯¸í„° gradient ìƒíƒœ ì§„ë‹¨."""
        attention_grads = sum(1 for p in self.model.attention_layer.parameters() if p.requires_grad)
        encoder_grads = sum(1 for p in self.model.shared_encoder.parameters() if p.requires_grad)  
        classifier_grads = sum(1 for p in self.model.cls_fc.parameters() if p.requires_grad)
        
        print(f"ğŸ” Gradient Status - Attention: {attention_grads}, Encoder: {encoder_grads}, Classifier: {classifier_grads}")
    
    def forward(
        self, 
        x: torch.Tensor, 
        return_features: bool = False
    ) -> Union[torch.Tensor, tuple]:
        """
        Fine-tuning forward pass.
        
        Args:
            x: Input EEG data
            return_features: ì¤‘ê°„ íŠ¹ì§• ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Classification logits ë˜ëŠ” (logits, features)
        """
        # ëª¨ë¸ì˜ forward pass (ë¶„ë¥˜ ì†ì‹¤ ì—†ì´ ë¡œì§“ë§Œ)
        x_pred, x_logits, _ = self.model.forward(x, None)
        
        if return_features:
            # íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•´ ì¸ì½”ë”ê¹Œì§€ë§Œ ì‹¤í–‰
            x_att = self.model.attention_layer(x, x.shape[0], self.time_steps)
            features, _, _ = self.model.shared_encoder(x_att)
            return x_logits, features
        else:
            return x_logits
    
    def training_step(self, batch, batch_idx):
        """DMMR ì›ë³¸ ë°©ì‹: 4D ë°ì´í„°ì—ì„œ subject_num ì¶• ê¸°ì¤€ í”¼í—˜ìë³„ ìˆœí™˜ Fine-tuning (Manual Optimization)"""
        optimizer = self.optimizers()
        
        source_batch_data, _, source_labels, _ = batch
        batch_size, _, _ = source_batch_data.shape

        total_loss = 0.0
        total_correct = 0
        total_samples = 0

        try:
            # Forward pass (ë¶„ë¥˜ìš© ëª¨ë¸ë§Œ ì‚¬ìš©)
            logits = self.forward(source_batch_data)

            # ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚°
            subject_loss = F.cross_entropy(
                logits, 
                source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            )
            
            # ğŸ”§ í•µì‹¬: Manual backward (ì›ë³¸ DMMR Fine-tuning ë°©ì‹ - í”¼í—˜ìë³„ ë…ë¦½ì  backward)
            self.manual_backward(subject_loss)
            
            # Loss ë° ì •í™•ë„ ëˆ„ì 
            total_loss += subject_loss.detach()
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì •í™•ë„ ëˆ„ì 
            preds = torch.argmax(logits, dim=1)
            actual_labels = source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            total_correct += (preds == actual_labels).sum().item()
            total_samples += actual_labels.size(0)
            
            # Debug info (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
            if batch_idx == 0:
                print(f"ğŸ¯ FineTune loss={subject_loss:.4f}")
                print(f"ğŸ” batch shapes: data={source_batch_data.shape}, labels={source_labels.shape}")

        except Exception as e:
            raise e
        
        # Optimizer step (ëª¨ë“  í”¼í—˜ì ì²˜ë¦¬ í›„ - ì›ë³¸ DMMR ë°©ì‹)
        optimizer.step()
        optimizer.zero_grad()
        
        avg_loss = total_loss
        accuracy = total_correct
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì „ì²´ ë°°ì¹˜ ê¸°ì¤€)
        self.train_metrics.update(preds, source_labels)
        
        # ë¡œê¹…
        self.log_dict({
            'train_loss': avg_loss,
            'train_acc': accuracy,
            'batch_size': batch_size
        }, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)
        
        return avg_loss
    
    def validation_step(self, batch, batch_idx):
        """DMMR ì›ë³¸ ë°©ì‹: 4D ë°ì´í„°ì—ì„œ subject_num ì¶• ê¸°ì¤€ í”¼í—˜ìë³„ ìˆœí™˜ Fine-tuning Validation"""
        
        source_batch_data, _, source_labels, _ = batch
        batch_size, _, _ = source_batch_data.shape
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        try:
            # Forward pass
            logits = self.forward(source_batch_data)
            
            # ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚°
            subject_loss = F.cross_entropy(
                logits,
                source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            )
            
            # Loss ë° ì •í™•ë„ ëˆ„ì 
            total_loss += subject_loss.detach()
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì •í™•ë„ ëˆ„ì 
            preds = torch.argmax(logits, dim=1)
            actual_labels = source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            total_correct += (preds == actual_labels).sum().item()
            total_samples += actual_labels.size(0)
            
            # Debug info (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
            if batch_idx == 0:
                print(f"ğŸ¯ FineTune loss={subject_loss:.4f}")
                print(f"ğŸ” batch shapes: data={source_batch_data.shape}, labels={source_labels.shape}")
            
        except Exception as e:
            raise e
        
        avg_loss = total_loss
        accuracy = total_correct
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì „ì²´ ë°°ì¹˜ ê¸°ì¤€)
        self.val_metrics.update(preds, source_labels)

        # ë¡œê¹…
        self.log_dict({
            'val_loss': avg_loss,
            'val_acc': accuracy,
            'val_batch_size': batch_size
        }, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)
        
        return avg_loss
    
    def test_step(self, batch, batch_idx):
        """DMMR ì „ìš© Test step - validation_stepê³¼ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„"""
        
        source_batch_data, _, source_labels, _ = batch
        batch_size, _, _ = source_batch_data.shape
        
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        all_predictions = []
        all_labels = []
        
        try:
            # Forward pass (testëŠ” gradient ê³„ì‚° ì—†ì´)
            with torch.no_grad():
                logits = self.forward(source_batch_data)

            # ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚°
            subject_loss = F.cross_entropy(
                logits, 
                source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            )
            
            # Loss ë° ì •í™•ë„ ëˆ„ì 
            total_loss += subject_loss.detach()
            
            # ì˜ˆì¸¡ê°’ ê³„ì‚° ë° ì •í™•ë„ ëˆ„ì 
            preds = torch.argmax(logits, dim=1)
            actual_labels = source_labels.squeeze() if source_labels.dim() > 1 else source_labels
            total_correct += (preds == actual_labels).sum().item()
            total_samples += actual_labels.size(0)
            
            # ì „ì²´ ì˜ˆì¸¡ê°’ê³¼ ë¼ë²¨ ì €ì¥ (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)
            all_predictions.extend(preds.cpu().numpy().tolist())
            all_labels.extend(actual_labels.cpu().numpy().tolist())
            
            # Debug info (ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
            if batch_idx == 0:
                print(f"ğŸ¯ FineTune loss={subject_loss:.4f}")
                print(f"ğŸ” batch shapes: data={source_batch_data.shape}, labels={source_labels.shape}")
            
        except Exception as e:
            raise e
        
        avg_loss = total_loss
        accuracy = total_correct
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (test ì „ìš©)
        # Convert to tensors for metric calculation
        all_predictions_tensor = torch.tensor(all_predictions, device=self.device)
        all_labels_tensor = torch.tensor(all_labels, device=self.device)
        
        # Test metrics update
        self.test_metrics.update(all_predictions_tensor, all_labels_tensor)
        
        # ë¡œê¹… (test ì „ìš©)
        self.log_dict({
            'test_loss': avg_loss,
            'test_acc': accuracy,
            'test_batch_size': batch_size
        }, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)
        
        return {
            'test_loss': avg_loss,
            'test_acc': accuracy,
            'predictions': all_predictions,
            'labels': all_labels,
        }
    
    def on_test_epoch_end(self):
        """Compute and log test metrics at the end of test epoch."""
        if hasattr(self, 'test_metrics'):
            # Compute metrics from metric collection
            test_metrics = self.test_metrics.compute()
            
            # Log all test metrics
            self.log_dict(test_metrics, prog_bar=True, sync_dist=True)
            
            # Reset metrics
            self.test_metrics.reset()
            
            print(f"ğŸ§ª Test completed. Final metrics: {test_metrics}")
    
    def on_validation_epoch_end(self):
        """Compute and log validation metrics at the end of each epoch."""
        # Compute metrics from metric collection
        val_metrics = self.val_metrics.compute()
        
        # Log all other metrics
        self.log_dict(val_metrics, prog_bar=False, sync_dist=True)
        
        # Reset metrics for next epoch
        self.val_metrics.reset()
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        """Prediction step for inference."""
        if isinstance(batch, (list, tuple)):
            x = batch[0]
        else:
            x = batch
        
        # Forward pass
        logits = self.forward(x)
        
        # í™•ë¥ ê³¼ ì˜ˆì¸¡ í´ë˜ìŠ¤ ë°˜í™˜
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)
        
        return {
            'predictions': preds,
            'probabilities': probs,
            'logits': logits
        }
    
    def freeze_pretrained_layers(self):
        """ì‚¬ì „í›ˆë ¨ ë ˆì´ì–´ ë™ê²°ì„ ìœ„í•œ ê³µê°œ ë©”ì„œë“œ."""
        self._freeze_pretrained_components()
    
    def unfreeze_pretrained_layers(self):
        """End-to-end í›ˆë ¨ì„ ìœ„í•œ ì‚¬ì „í›ˆë ¨ ë ˆì´ì–´ í•´ë™."""
        self._unfreeze_pretrained_components()
    
    def get_feature_representations(self, x: torch.Tensor) -> torch.Tensor:
        """ì¸ì½”ë”ë¡œë¶€í„° íŠ¹ì§• í‘œí˜„ ì¶”ì¶œ."""
        with torch.no_grad():
            x_att = self.model.attention_layer(x, x.shape[0], self.time_steps)
            features, _, _ = self.model.shared_encoder(x_att)
        
        return features