"""
DMMR Pre-training Model.

Domain-invariant representation learning with mixed reconstruction.
"""
from typing import Tuple
import torch
import torch.nn as nn
import torch.nn.functional as F

from ..layers import Attention, Encoder, Decoder, DomainClassifier, ReverseLayerF
from ..utils import MSE, timeStepsShuffle


class DMMRPreTrainingModel(nn.Module):
    """
    DMMR Pre-training Model.
    
    DMMR ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë¡œ ë„ë©”ì¸ ë¶ˆë³€ í‘œí˜„ í•™ìŠµê³¼ 
    í˜¼í•© ì¬êµ¬ì„±ì„ í†µí•œ robust representationì„ í•™ìŠµí•©ë‹ˆë‹¤.
    
    Args:
        number_of_source: ì†ŒìŠ¤ ë„ë©”ì¸ ìˆ˜ (default: 14, í”¼í—˜ì ìˆ˜)
        number_of_category: í´ë˜ìŠ¤ ìˆ˜ (default: 3)  
        batch_size: ë°°ì¹˜ í¬ê¸° (default: 10)
        time_steps: ì‹œê°„ ë‹¨ê³„ ìˆ˜ (default: 15)
        input_dim: ì…ë ¥ ì°¨ì› (default: 310, EEG features)
    """
    
    def __init__(
        self,
        number_of_source: int = 14,
        number_of_category: int = 3,
        batch_size: int = 10,
        time_steps: int = 15,
        input_dim: int = 310
    ) -> None:
        super().__init__()
        self.batch_size = batch_size
        self.time_steps = time_steps
        self.number_of_source = number_of_source
        self.input_dim = input_dim
        # Core components
        self.attention_layer = Attention(input_dim=input_dim)
        self.shared_encoder = Encoder(input_dim=input_dim, hid_dim=64, n_layers=1)
        self.domain_classifier = DomainClassifier(input_dim=64, output_dim=number_of_source)
        
        # Loss functions
        self.mse = MSE()
        
        # Decoders for each source domain (ì•ˆì „í•œ êµ¬í˜„)
        self.decoders = nn.ModuleList([
            Decoder(input_dim=input_dim, hid_dim=64, n_layers=1, output_dim=input_dim)
            for _ in range(number_of_source)
        ])

    def forward(
        self, 
        x: torch.Tensor, 
        corres: torch.Tensor, 
        subject_id: torch.Tensor, 
        m: float = 0.0, 
        mark: int = 0,
        subject_mask: torch.Tensor = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        DMMR Pre-training forward pass.
        
        Args:
            x: Input data [batch_size, time_steps, input_dim]
            corres: Correspondence data for reconstruction supervision
            subject_id: Subject IDs for domain classification
            m: Gradient reversal multiplier (default: 0.0)
            mark: Training phase marker (default: 0)
            subject_mask: Subject mask data for single-label subject data case
            
        Returns:
            Tuple of (reconstruction_loss, similarity_loss)
        """
        # Noise Injection: Time Steps Shuffling
        x = timeStepsShuffle(x)
        
        # Attention-Based Pooling (ABP) module
        x = self.attention_layer(x, x.shape[0], self.time_steps)
        
        # Encode weighted features with shared encoder
        shared_last_out, shared_hn, shared_cn = self.shared_encoder(x)
        
        # Domain Adversarial Training (DG_DANN module)
        # Gradient Reversal Layer
        reverse_feature = ReverseLayerF.apply(shared_last_out, m)
        
        # Subject/Domain Discriminator
        subject_predict = self.domain_classifier(reverse_feature)
        subject_predict = F.log_softmax(subject_predict, dim=1)
        
        # Domain adversarial loss
        sim_loss = F.nll_loss(subject_predict, subject_id)
        
        # Build supervision for decoders
        # print(f"ğŸ” Correspondence data shape: {corres.shape}")
        # print(f"ğŸ” Expected batch_size from corres.shape[0]: {corres.shape[0]}")
        # print(f"ğŸ” Input x shape: {x.shape}")
        # print(f"ğŸ” Model time_steps: {self.time_steps}")
        
        # Attention layerë¡œ correspondence ë°ì´í„° ì²˜ë¦¬
        # subject_mask ê°’ìœ¼ë¡œ correspondence ë°ì´í„° í•„í„°ë§
        # corres: (batch_size*subject_num, time_steps, features), subject_mask: (batch_size, subject_num)
        splitted_corres = torch.chunk(corres, self.number_of_source, dim=0)

        # subject_mask: (batch_size, subject_num)
        # ë°°ì¹˜ ì „ì²´ì—ì„œ í•˜ë‚˜ë¼ë„ ìœ íš¨í•˜ë©´ í•´ë‹¹ subjectëŠ” ìœ íš¨
        # batch_sizeì˜ subjectë³„ mask ê°’ì€ ê°™ìŒ
        valid_subjects_mask = torch.any(subject_mask > 0, dim=0)  # (subject_num,)
        valid_subject_indices = torch.where(valid_subjects_mask)[0]

        # 3ë‹¨ê³„: ìœ íš¨í•œ correspondenceë§Œ ì„ íƒ
        valid_corres_chunks = [splitted_corres[i] for i in valid_subject_indices]
        
        # 4ë‹¨ê³„: ìœ íš¨í•œ correspondenceë§Œ concatenateí•˜ì—¬ attention ì ìš©
        valid_corres = torch.cat(valid_corres_chunks, dim=0)
        # print(f"ğŸ” Valid correspondence shape before attention: {valid_corres.shape}")
        
        # Attentionì€ ìœ íš¨í•œ ë°ì´í„°ì—ë§Œ ì ìš©
        valid_corres = self.attention_layer(valid_corres, valid_corres.shape[0], self.time_steps)
        # print(f"ğŸ” Valid correspondence shape after attention: {valid_corres.shape}")

        # ë‹¤ì‹œ subjectë³„ë¡œ ë¶„í• 
        splitted_tensors = torch.chunk(valid_corres, len(valid_subject_indices), dim=0)

        # First stage: Reconstruct features and create mixed features
        rec_loss = 0
        mix_subject_feature = 0

        # subject_maskê°€ 1ì¸ decoderë§Œì„ ì‚¬ìš©
        for i, subject_idx in enumerate(valid_subject_indices):
            # Reconstruct features in first stage
            x_out, *_ = self.decoders[subject_idx](shared_last_out, shared_hn, shared_cn, self.time_steps)
            # Mix method for data augmentation
            mix_subject_feature += x_out
        
        # for i, decoder in enumerate(self.decoders):
        #     # Reconstruct features in first stage
        #     x_out, *_ = decoder(shared_last_out, shared_hn, shared_cn, self.time_steps)
        #     # Mix method for data augmentation
        #     mix_subject_feature += x_out
        
        # Second stage: Re-encode mixed features
        shared_last_out_2, shared_hn_2, shared_cn_2 = self.shared_encoder(mix_subject_feature)
        
        # Second stage: Reconstruct and compute loss
        # subject_maskê°€ 1ì¸ decoderë§Œì„ ì‚¬ìš©
        for i, subject_idx in enumerate(valid_subject_indices):
            # Reconstruct features in second stage
            x_out, *_ = self.decoders[subject_idx](shared_last_out_2, shared_hn_2, shared_cn_2, self.time_steps)
            # Compute reconstruction loss only in second stage
            rec_loss += self.mse(x_out, splitted_tensors[i])

        # for i, decoder in enumerate(self.decoders):
            # x_out, *_ = decoder(shared_last_out_2, shared_hn_2, shared_cn_2, self.time_steps)
            # # Compute reconstruction loss only in second stage
            # rec_loss += self.mse(x_out, splitted_tensors[i])
        
        return rec_loss, sim_loss