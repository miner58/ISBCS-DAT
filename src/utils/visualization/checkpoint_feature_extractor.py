"""
RayTune ì²´í¬í¬ì¸íŠ¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ ë° ì €ì¥ì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
1. checkpoint_analysis_results.csvì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ë¡œë“œ
2. ì²´í¬í¬ì¸íŠ¸ë¡œë¶€í„° ëª¨ë¸ ë³µì›
3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ ë° ì €ì¥
4. ì¶”ì¶œëœ íŠ¹ì§•, íƒ€ê²Ÿ ë ˆì´ë¸”, ë„ë©”ì¸ ë ˆì´ë¸”, ì…ë ¥ ë°ì´í„° ë“±ì„ NPZ íŒŒì¼ë¡œ ì €ì¥
ë°˜í™˜ íƒ€ì…ì´ Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]ë¡œ ë³€ê²½ (ì…ë ¥ ë°ì´í„° ì¶”ê°€)
ì¶œë ¥ ì •ë³´ ê°œì„ :
ì…ë ¥ ë°ì´í„° shape ì •ë³´ë„ ì¶œë ¥í•˜ë„ë¡ ì¶”ê°€
ì´ì œ NPZ íŒŒì¼ì—ëŠ” ë‹¤ìŒ ë°ì´í„°ë“¤ì´ ì €ì¥:
    features: ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„°
    target_labels: íƒ€ê²Ÿ ë ˆì´ë¸”
    domain_labels: ë„ë©”ì¸ ë ˆì´ë¸” (ìˆëŠ” ê²½ìš°)
    input_data: ì›ë³¸ ì…ë ¥ ë°ì´í„°
    subject_name: í”¼í—˜ì ì´ë¦„
    model_name: ëª¨ë¸ ì´ë¦„
    checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
"""
import os
import sys
import json
import glob
import yaml
import torch
import pandas as pd
import numpy as np
import pytorch_lightning as pl
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

# ëª¨ë¸ ë° ë°ì´í„° ëª¨ë“ˆ import
from src.models.eegnet import EEGNet
from src.models.eegnet_grl import EEGNetGRL, EEGNetLNL, EEGNetMI, EEGNetLNLAutoCorrelation
from src.models.eegnetDRO import EEGNetDRO
from src.models.eegnet_grl_lag import EEGNetLNLLag
from src.data.modules.EEGdataModuel import EEGDataModule


class FeatureExtractor:
    """ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, test_data_default_path: str):
        self.test_data_default_path = test_data_default_path
        self.model_dict = {
            'EEGNet': EEGNet,
            'EEGNetDomainAdaptation_LNL': EEGNetLNL,
            'EEGNetDomainAdaptation_Not_GRL': EEGNetMI,
            'EEGNetDomainAdaptation_Only_GRL': EEGNetGRL,
            'EEGNetLNL': EEGNetLNL,
            'EEGNetMI': EEGNetMI,
            'EEGNetGRL': EEGNetGRL,
            'EEGNetLNLLag': EEGNetLNLLag,
            'EEGNetLNLAutoCorrelation': EEGNetLNLAutoCorrelation,
            'EEGNetDRO': EEGNetDRO
        }
    
    def _load_cortical_regions(self, regions_path: str):
        """
        cortical_regions.json íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜
        :param regions_path: regions.json íŒŒì¼ ê²½ë¡œ
        :return: List[List[int]] í˜•íƒœì˜ cortical regions
        """
        if regions_path is None:
            raise ValueError("Cortical regions path is required for data augmentation.")
        
        with open(regions_path, 'r') as f:
            regions = json.load(f)
        
        regions_list = []
        for key, values in regions.items():
            regions_list.append(values)

        return regions_list

    def _prepare_augmentation_config_for_datamodule(self, da_config: dict):
        """
        EEGDataModuleì— ì „ë‹¬í•  ë°ì´í„° ì¦ê°• ì„¤ì •ì„ ìµœì¢…ì ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
        YAMLì—ì„œ ë¡œë“œëœ ì„¤ì •ê³¼, í•„ìš”í•œ ê²½ìš° ì¶”ê°€ì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.

        :param da_config: YAML íŒŒì¼ì—ì„œ ë¡œë“œëœ ë°ì´í„° ì¦ê°• ì„¤ì •
        :return: EEGDataModuleì˜ data_augmentation_config íŒŒë¼ë¯¸í„°ì— ì „ë‹¬ë  ìµœì¢… ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        final_config = {
            'enabled': da_config.get('enabled', True),
            'train_only': da_config.get('train_only', True),
            'methods': []
        }

        # ì„¤ì •ëœ ë©”ì„œë“œ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° í•„ìš”í•œ ì •ë³´ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        for method_name in da_config.get('methods', []):
            method_setting = da_config.get('setting', {}).get(method_name)
            if not method_setting:
                print(f"Warning: Augmentation method '{method_name}' has no setting. Skipping.")
                continue

            method_info = {
                'type': method_setting.get('name'), # 'cortical' ë˜ëŠ” 'subject'
                'prob_method': method_setting.get('swap_probability_method', 'uniform')
            }

            # CorticalRegionChannelSwapì˜ ê²½ìš°, regions ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
            if method_name == 'CorticalRegionChannelSwap':
                regions_path = method_setting.get('cortical_regions_path')
                if not regions_path:
                    raise ValueError("cortical_regions_path is required for CorticalRegionChannelSwap.")
                
                # ì‹¤ì œ ê²½ë¡œëŠ” default_pathì™€ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
                full_regions_path = os.path.join(self.test_data_default_path, regions_path)
                method_info['regions'] = self._load_cortical_regions(full_regions_path)
            
            # SubjectLevelChannelSwapì˜ ê²½ìš°, enable_soft_labels ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.
            elif method_name == 'SubjectLevelChannelSwap':
                method_info['enable_soft_labels'] = method_setting.get('enable_soft_labels', False)

            final_config['methods'].append(method_info)
            
        return final_config

    def create_data_augmentation_config(self, da_config_path: str = None):
        """
        ë°ì´í„° ì¦ê°• ì„¤ì •ì„ ìœ„í•œ config ìƒì„±
        1. da_config_pathì— ì§€ì •ëœ YAML íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        2. ë¡œë“œëœ ì„¤ì •ì„ EEGDataModuleì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        :param da_config_path: ë°ì´í„° ì¦ê°• ì„¤ì • YAML íŒŒì¼ ê²½ë¡œ
        :return: EEGDataModuleì— ì „ë‹¬í•  ë°ì´í„° ì¦ê°• ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        if not da_config_path:
            return {'enabled': False}  # ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”
        
        full_config_path = os.path.join(self.test_data_default_path, da_config_path)
        print(f"ğŸ“‹ ë°ì´í„° ì¦ê°• ì„¤ì • ë¡œë“œ: {full_config_path}")
        
        try:
            with open(full_config_path, 'r') as f:
                loaded_da_config = yaml.safe_load(f)
            
            # YAMLì—ì„œ ë¡œë“œëœ ì„¤ì •ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜ (Ray Tune ì„¤ì • ìŠ¤íƒ€ì¼ ì²˜ë¦¬)
            mapped_da_config = {}
            for key, value in loaded_da_config.items():
                if isinstance(value, dict) and 'value' in value:
                    mapped_da_config[key] = value['value']
                else:
                    mapped_da_config[key] = value
            
            # EEGDataModuleì— ì „ë‹¬í•˜ê¸° ìœ„í•´ ìµœì¢…ì ìœ¼ë¡œ ì„¤ì • í¬ë§·íŒ…
            final_da_config = self._prepare_augmentation_config_for_datamodule(mapped_da_config)
            final_da_config['enabled'] = True  # ìµœì¢…ì ìœ¼ë¡œ ì¦ê°• í™œì„±í™” ìƒíƒœ ëª…ì‹œ

            print(f"âœ… ë°ì´í„° ì¦ê°• ì„¤ì • ì™„ë£Œ: {final_da_config}")
            return final_da_config
            
        except FileNotFoundError:
            print(f"âŒ ë°ì´í„° ì¦ê°• ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_config_path}")
            return {'enabled': False}
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì¦ê°• ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {'enabled': False}
    
    def load_model_from_checkpoint(self, checkpoint_path: str, model_name: str):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        model_class = self.model_dict.get(model_name)
        if not model_class:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
        
        ckpt_file = os.path.join(checkpoint_path, "checkpoint.ckpt")
        if not os.path.exists(ckpt_file):
            raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {ckpt_file}")
        
        print(f"ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ: {ckpt_file}")
        model = model_class.load_from_checkpoint(ckpt_file)
        model.eval()
        return model
    
    def create_data_module(self, test_config_path: str, batch_size: int = 16, data_augmentation_config: dict = None):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë“ˆ ìƒì„±"""
        # ê²½ë¡œ ë³´ì •
        config_path = test_config_path
        old_base = "/mnt/sdb1/jsw/hanseul/code/Fairness_for_generalization"
        new_base = "/home/jsw/Fairness/Fairness_for_generalization"
        
        if config_path.startswith(old_base):
            relative_path = os.path.relpath(config_path, old_base)
            config_path = os.path.join(new_base, relative_path)
        
        with open(config_path, 'r') as f:
            data_config = json.load(f)
        
        data_module = EEGDataModule(
            data_config=data_config, batch_size=batch_size, masking_ch_list=[],
            rm_ch_list=[], subject_usage="test1", seed=None,
            default_path=self.test_data_default_path, skip_time_list=None,
            data_augmentation_config=data_augmentation_config
        )
        
        data_module.setup('test')
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data_module.test_dataset)}ê°œ ìƒ˜í”Œ")
        return data_module
    
    def extract_features(self, model, data_module) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        model.eval()
        all_features = []
        all_target_labels = []
        all_domain_labels = []
        all_input_data = []
        
        with torch.no_grad():
            for batch in data_module.test_dataloader():
                # ë°°ì¹˜ êµ¬ì¡° í™•ì¸: (x, target_labels) ë˜ëŠ” (x, target_labels, domain_labels)
                if len(batch) == 2:
                    x, target_labels = batch
                    domain_labels = None
                elif len(batch) == 3:
                    x, target_labels, domain_labels = batch
                else:
                    x = batch[0]
                    target_labels = batch[1]
                    domain_labels = batch[2] if len(batch) > 2 else None
                
                # ì…ë ¥ ë°ì´í„° ì €ì¥ (GPUë¡œ ì´ë™ ì „ì— ì €ì¥)
                all_input_data.append(x.cpu().numpy())
                
                # GPU ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì´ë™
                if torch.cuda.is_available():
                    x = x.cuda()
                    model = model.cuda()
                
                # íŠ¹ì§• ì¶”ì¶œ (extract_features ë©”ì„œë“œ ì‚¬ìš©)
                if hasattr(model, 'extract_features'):
                    features = model.extract_features(x)
                else:
                    # extract_featuresê°€ ì—†ëŠ” ê²½ìš° forwardë¡œ íŠ¹ì§• ì¶”ì¶œ
                    features = model.feature_extractor(x.permute(0, 3, 1, 2))
                    features = torch.flatten(features, start_dim=1)
                
                all_features.append(features.cpu().numpy())
                all_target_labels.append(target_labels.numpy())
                
                # domain_labelsê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if domain_labels is not None:
                    all_domain_labels.append(domain_labels.numpy())
        
        features_array = np.vstack(all_features)
        target_labels_array = np.hstack(all_target_labels)
        domain_labels_array = np.hstack(all_domain_labels) if all_domain_labels else None
        input_data_array = np.vstack(all_input_data)
        
        print(f"âœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {features_array.shape}")
        print(f"   Input data: {input_data_array.shape}")
        print(f"   Target labels: {target_labels_array.shape}, ê³ ìœ ê°’: {np.unique(target_labels_array)}")
        if domain_labels_array is not None:
            print(f"   Domain labels: {domain_labels_array.shape}, ê³ ìœ ê°’: {np.unique(domain_labels_array)}")
        
        return features_array, target_labels_array, domain_labels_array, input_data_array

    def extract_features_with_predictions(self, model, data_module) -> Dict[str, np.ndarray]:
        """íŠ¹ì§• + ì˜ˆì¸¡ ê²°ê³¼ ë™ì‹œ ì¶”ì¶œ"""
        model.eval()
        all_features = []
        all_predictions = []
        all_prediction_probs = []
        all_target_labels = []
        all_domain_labels = []
        all_input_data = []
        
        with torch.no_grad():
            for batch in data_module.test_dataloader():
                # ë°°ì¹˜ êµ¬ì¡° í™•ì¸
                if len(batch) == 2:
                    x, target_labels = batch
                    domain_labels = None
                elif len(batch) == 3:
                    x, target_labels, domain_labels = batch
                else:
                    x = batch[0]
                    target_labels = batch[1]
                    domain_labels = batch[2] if len(batch) > 2 else None
                
                # ì…ë ¥ ë°ì´í„° ì €ì¥ (GPUë¡œ ì´ë™ ì „ì— ì €ì¥)
                all_input_data.append(x.cpu().numpy())
                
                # GPU ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì´ë™
                if torch.cuda.is_available():
                    x = x.cuda()
                    model = model.cuda()
                
                # íŠ¹ì§• ì¶”ì¶œ
                if hasattr(model, 'extract_features'):
                    features = model.extract_features(x)
                else:
                    # extract_featuresê°€ ì—†ëŠ” ê²½ìš° feature_extractor ì‚¬ìš©
                    features = model.feature_extractor(x.permute(0, 3, 1, 2))
                    features = torch.flatten(features, start_dim=1)
                
                # ì˜ˆì¸¡ ìˆ˜í–‰ (ì „ì²´ forward pass)
                model_output = model(x)
                # ëª¨ë¸ ì¶œë ¥ì´ tupleì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ(logits)ë§Œ ì‚¬ìš©
                if isinstance(model_output, tuple):
                    logits = model_output[0]
                else:
                    logits = model_output
                    
                predictions = torch.argmax(logits, dim=1)
                prediction_probs = torch.softmax(logits, dim=1)
                
                # ê²°ê³¼ ì €ì¥
                all_features.append(features.cpu().numpy())
                all_predictions.append(predictions.cpu().numpy())
                all_prediction_probs.append(prediction_probs.cpu().numpy())
                all_target_labels.append(target_labels.cpu().numpy())
                
                # domain_labelsê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if domain_labels is not None:
                    all_domain_labels.append(domain_labels.cpu().numpy())
        
        # ë°°ì—´ ê²°í•©
        features_array = np.vstack(all_features)
        predictions_array = np.hstack(all_predictions)
        prediction_probs_array = np.vstack(all_prediction_probs)
        target_labels_array = np.hstack(all_target_labels)
        domain_labels_array = np.hstack(all_domain_labels) if all_domain_labels else None
        input_data_array = np.vstack(all_input_data)
        
        # ì •í™•ë„ ê³„ì‚°
        correct_predictions = predictions_array == target_labels_array
        accuracy = np.mean(correct_predictions)
        
        # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°
        per_class_accuracy = {}
        for class_id in np.unique(target_labels_array):
            class_mask = target_labels_array == class_id
            if np.sum(class_mask) > 0:
                per_class_accuracy[int(class_id)] = np.mean(correct_predictions[class_mask])
        
        print(f"âœ… íŠ¹ì§• + ì˜ˆì¸¡ ì¶”ì¶œ ì™„ë£Œ: {features_array.shape}")
        print(f"   ì „ì²´ ì •í™•ë„: {accuracy:.4f}")
        print(f"   í´ë˜ìŠ¤ë³„ ì •í™•ë„: {per_class_accuracy}")
        print(f"   ì˜ˆì¸¡ ë¶„í¬: {np.bincount(predictions_array)}")
        print(f"   ì‹¤ì œ ë¶„í¬: {np.bincount(target_labels_array)}")
        
        return {
            'features': features_array,
            'predictions': predictions_array,
            'prediction_probs': prediction_probs_array,
            'target_labels': target_labels_array,
            'domain_labels': domain_labels_array,
            'input_data': input_data_array,
            'correct_predictions': correct_predictions,
            'accuracy': accuracy,
            'per_class_accuracy': per_class_accuracy
        }

    def extract_augmented_features(self, model, data_module_original, data_module_augmented) -> Dict:
        """ì›ë³¸ê³¼ ì¦ê°•ëœ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        print("ğŸ”„ ì›ë³¸ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ")
        orig_features, orig_target_labels, orig_domain_labels, orig_input_data = self.extract_features(model, data_module_original)
        
        print("ğŸ”„ ì¦ê°•ëœ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ")
        aug_features, aug_target_labels, aug_domain_labels, aug_input_data = self.extract_features(model, data_module_augmented)
        
        return {
            'original': {
                'features': orig_features,
                'target_labels': orig_target_labels,
                'domain_labels': orig_domain_labels,
                'input_data': orig_input_data
            },
            'augmented': {
                'features': aug_features,
                'target_labels': aug_target_labels,
                'domain_labels': aug_domain_labels,
                'input_data': aug_input_data
            }
        }


class CheckpointFeatureRunner:
    """ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ ì‹¤í–‰ê¸°"""
    
    def __init__(self, analysis_result_path: str, test_config_base_path: str, test_data_default_path: str):
        self.analysis_result_path = analysis_result_path
        self.test_config_base_path = test_config_base_path
        self.extractor = FeatureExtractor(test_data_default_path)
    
    def load_analysis_results(self) -> pd.DataFrame:
        """ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        df = pd.read_csv(self.analysis_result_path)
        valid_df = df[df['checkpoint_found'] == True].copy()
        print(f"ğŸ“‹ ìœ íš¨í•œ ì²´í¬í¬ì¸íŠ¸: {len(valid_df)}ê°œ")
        return valid_df
    
    def get_test_config_files(self, subject_name: str) -> List[str]:
        """í…ŒìŠ¤íŠ¸ ì„¤ì • íŒŒì¼ ì°¾ê¸°"""
        subject_path = os.path.join(self.test_config_base_path, subject_name)
        if not os.path.exists(subject_path):
            return []
        return glob.glob(os.path.join(subject_path, '**', '*.json'), recursive=True)
    
    def extract_model_info(self, experiment_path: str) -> Dict:
        """ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
        yaml_files = glob.glob(os.path.join(os.path.dirname(experiment_path), '*.yml'))
        if not yaml_files:
            return {'model_name': 'EEGNetLNL', 'batch_size': 16}
        
        try:
            with open(yaml_files[0], 'r') as f:
                config = yaml.safe_load(f)
            search_space = config.get('search_space', {})
            return {
                'model_name': search_space.get('model_name', 'EEGNetLNL'),
                'batch_size': search_space.get('batch_size', 16)
            }
        except:
            return {'model_name': 'EEGNetLNL', 'batch_size': 16}
    
    def run_single_extraction(self, checkpoint_info: pd.Series, output_dir: str, 
                             data_augmentation_config_path: str = None,
                             include_predictions: bool = True) -> bool:
        """ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ íŠ¹ì§• ì¶”ì¶œ"""
        subject_name = checkpoint_info['test_subject_name']
        checkpoint_path = checkpoint_info['checkpoint_path']
        experiment_path = checkpoint_info['experiment_path']
        
        print(f"\nğŸ¯ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘: {subject_name}")
        
        # ëª¨ë¸ ì •ë³´ ë° ì„¤ì • íŒŒì¼
        model_info = self.extract_model_info(experiment_path)
        test_config_files = self.get_test_config_files(subject_name)
        
        if not test_config_files:
            print(f"âŒ {subject_name} í…ŒìŠ¤íŠ¸ ì„¤ì • ì—†ìŒ")
            return False
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model = self.extractor.load_model_from_checkpoint(checkpoint_path, model_info['model_name'])
            
            # ë°ì´í„° ì¦ê°• ì„¤ì • ìƒì„± (ì„ íƒì‚¬í•­)
            data_augmentation_config = None
            if data_augmentation_config_path:
                data_augmentation_config = self.extractor.create_data_augmentation_config(data_augmentation_config_path)
            
            # ë°ì´í„° ëª¨ë“ˆ ìƒì„±
            data_module = self.extractor.create_data_module(
                test_config_files[0], 
                model_info['batch_size'],
                data_augmentation_config
            )
            
            # íŠ¹ì§• ì¶”ì¶œ (ì˜ˆì¸¡ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°)
            if include_predictions:
                print(f"ğŸ” ì˜ˆì¸¡ ì •ë³´ í¬í•¨ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
                extraction_result = self.extractor.extract_features_with_predictions(model, data_module)
                
                # ì €ì¥í•  ë°ì´í„° êµ¬ì„± (ì˜ˆì¸¡ ì •ë³´ í¬í•¨)
                save_data = {
                    'features': extraction_result['features'],
                    'target_labels': extraction_result['target_labels'],
                    'predictions': extraction_result['predictions'],
                    'prediction_probs': extraction_result['prediction_probs'],
                    'correct_predictions': extraction_result['correct_predictions'],
                    'input_data': extraction_result['input_data'],
                    'accuracy': extraction_result['accuracy'],
                    'per_class_accuracy': str(extraction_result['per_class_accuracy']),
                    'subject_name': subject_name,
                    'model_name': model_info['model_name'],
                    'checkpoint_path': checkpoint_path
                }
                
                # domain_labelsê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if extraction_result['domain_labels'] is not None:
                    save_data['domain_labels'] = extraction_result['domain_labels']
                
                output_filename = f"{subject_name}_features_with_predictions.npz"
                
            else:
                print(f"ğŸ” ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
                features, target_labels, domain_labels, input_data = self.extractor.extract_features(model, data_module)
                
                # ì €ì¥í•  ë°ì´í„° êµ¬ì„± (ê¸°ì¡´ ë°©ì‹)
                save_data = {
                    'features': features, 
                    'target_labels': target_labels,
                    'input_data': input_data,
                    'subject_name': subject_name,
                    'model_name': model_info['model_name'],
                    'checkpoint_path': checkpoint_path
                }
                
                # domain_labelsê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if domain_labels is not None:
                    save_data['domain_labels'] = domain_labels
                
                output_filename = f"{subject_name}_features.npz"
            
            # ì €ì¥
            output_file = os.path.join(output_dir, output_filename)
            os.makedirs(output_dir, exist_ok=True)
            
            # ë°ì´í„° ì¦ê°• ì •ë³´ ì €ì¥
            if data_augmentation_config and data_augmentation_config.get('enabled', False):
                save_data['data_augmentation_enabled'] = True
                save_data['data_augmentation_config'] = str(data_augmentation_config)
            
            np.savez_compressed(output_file, **save_data)
            
            if include_predictions:
                print(f"ğŸ’¾ ì˜ˆì¸¡ í¬í•¨ íŠ¹ì§• ì €ì¥: {output_file}")
                print(f"   ì •í™•ë„: {extraction_result['accuracy']:.4f}")
            else:
                print(f"ğŸ’¾ íŠ¹ì§• ì €ì¥: {output_file} (shape: {features.shape})")
            
            return True
            
        except Exception as e:
            print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return False
    
    def run_all_extractions(self, output_base_dir: str, max_extractions: Optional[int] = None,
                           data_augmentation_config_path: str = None,
                           include_predictions: bool = True) -> Dict:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ íŠ¹ì§• ì¶”ì¶œ"""
        print("ğŸš€ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘\n")
        
        if data_augmentation_config_path:
            print(f"ğŸ“‹ ë°ì´í„° ì¦ê°• ì„¤ì • ì‚¬ìš©: {data_augmentation_config_path}")
        
        if include_predictions:
            print(f"ğŸ” ì˜ˆì¸¡ ì •ë³´ í¬í•¨ ì¶”ì¶œ ëª¨ë“œ")
        else:
            print(f"ğŸ” ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ëª¨ë“œ")
        
        valid_checkpoints = self.load_analysis_results()
        if max_extractions:
            valid_checkpoints = valid_checkpoints.head(max_extractions)
        
        results = {'success': [], 'failed': []}
        
        for idx, checkpoint_info in valid_checkpoints.iterrows():
            subject_name = checkpoint_info['test_subject_name']
            output_dir = os.path.join(output_base_dir, subject_name)
            
            if self.run_single_extraction(checkpoint_info, output_dir, 
                                        data_augmentation_config_path, include_predictions):
                results['success'].append(subject_name)
            else:
                results['failed'].append(subject_name)
        
        print(f"\nğŸ“Š íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: ì„±ê³µ {len(results['success'])}ê°œ, ì‹¤íŒ¨ {len(results['failed'])}ê°œ")
        return results


def extract_checkpoint_features(analysis_result_path: str, test_config_base_path: str, 
                               test_data_default_path: str, output_dir: str, 
                               max_extractions: Optional[int] = None,
                               data_augmentation_config_path: str = None,
                               include_predictions: bool = True) -> Dict:
    """ì²´í¬í¬ì¸íŠ¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤í–‰ í•¨ìˆ˜"""
    runner = CheckpointFeatureRunner(
        analysis_result_path=analysis_result_path,
        test_config_base_path=test_config_base_path,
        test_data_default_path=test_data_default_path
    )
    
    return runner.run_all_extractions(output_dir, max_extractions, 
                                    data_augmentation_config_path, include_predictions)


if __name__ == "__main__":
    print("ğŸš€ ì²´í¬í¬ì¸íŠ¸ íŠ¹ì§• ì¶”ì¶œ ì‹œìŠ¤í…œ")
    
    # ê¸°ë³¸ ì„¤ì •
    analysis_result_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_subject_60/1_inner_wireless/ray_results_test1_Day1,8_finetune_ReduceLROnPlateau_LNL_batch16/analyzing_result/checkpoint_analysis_results.csv"
    test_config_base_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization/data/raw5&6config/test/only1Day1,8"
    test_data_default_path = "/home/jsw/Fairness/tmp/Fairness_for_generalization"
    output_dir = "/home/jsw/Fairness/tmp/Fairness_for_generalization/results_trans/results_trans/results_subject_60/1_inner_wireless/ray_results_test1_Day1,8_finetune_ReduceLROnPlateau_LNL_batch16/analyzing_result/extracted_features"
    
    # ë°ì´í„° ì¦ê°• ì„¤ì • (ì„ íƒì‚¬í•­)
    data_augmentation_config_path = "config/RayTune/raw5&6/data_augmentation/data_augmentation.yml"
    
    try:
        results = extract_checkpoint_features(
            analysis_result_path=analysis_result_path,
            test_config_base_path=test_config_base_path,
            test_data_default_path=test_data_default_path,
            output_dir=output_dir,
            max_extractions=3,
            data_augmentation_config_path=data_augmentation_config_path
        )
        print(f"âœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ: {results}")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
